# -*-makefile-*-
#
# model configurations
#



# SRCLANGS = da no sv
# TRGLANGS = fi

SRCLANGS = sv
TRGLANGS = fi

ifndef SRC
  SRC := ${firstword ${SRCLANGS}}
endif
ifndef TRG
  TRG := ${lastword ${TRGLANGS}}
endif


# sorted languages and langpair used to match resources in OPUS
SORTLANGS = $(sort ${SRC} ${TRG})
SPACE     = $(empty) $(empty)
LANGPAIR  = ${firstword ${SORTLANGS}}-${lastword ${SORTLANGS}}
LANGSTR   = ${subst ${SPACE},+,$(SRCLANGS)}-${subst ${SPACE},+,$(TRGLANGS)}


## for same language pairs: add numeric extension
ifeq (${SRC},$(TRG))
  SRCEXT = ${SRC}1
  TRGEXT = ${SRC}2
else
  SRCEXT = ${SRC}
  TRGEXT = ${TRG}
endif


## all of OPUS
OPUSCORPORA  = ${patsubst %/latest/moses/${LANGPAIR}.txt.zip,%,\
		${patsubst ${OPUSHOME}/%,%,\
		${shell ls ${OPUSHOME}/*/latest/moses/${LANGPAIR}.txt.zip}}}

ALL_LANG_PAIRS = ${shell ls ${WORKHOME} | grep -- '-' | grep -v old}


## train/dev/test data

TRAINSET = $(filter-out WMT-News ${DEVSET} ${TESTSET},${OPUSCORPORA})
DEVSET   = Tatoeba
TESTSET  = ${DEVSET}
TUNESET  = OpenSubtitles


## size of dev data, test data and BPE merge operations

DEVSIZE     = 5000
DEVMINSIZE  = 1000
TESTSIZE    = 5000
HELDOUTSIZE = ${DEVSIZE}

BPESIZE    = 32000
SRCBPESIZE = ${BPESIZE}
TRGBPESIZE = ${BPESIZE}

ifndef VOCABSIZE
  VOCABSIZE  = $$((${SRCBPESIZE} + ${TRGBPESIZE} + 1000))
endif



## pre-processing type
PRE_SRC   = bpe${SRCBPESIZE:000=}k
PRE_TRG   = bpe${TRGBPESIZE:000=}k

##-------------------------------------
## name of the data set (and the model)
##  - single corpus = use that name
##  - multiple corpora = opus
## add also vocab size to the name
##-------------------------------------

ifndef DATASET
ifeq (${words ${TRAINSET}},1)
  DATASET = ${TRAINSET}
else
  DATASET = opus
endif
endif



## DATADIR = directory where the train/dev/test data are
## WORKDIR = directory used for training

DATADIR = ${WORKHOME}/data
WORKDIR = ${WORKHOME}/${LANGSTR}


## data sets
TRAIN_BASE = ${WORKDIR}/train/${DATASET}
TRAIN_SRC  = ${TRAIN_BASE}.src
TRAIN_TRG  = ${TRAIN_BASE}.trg

## training data in local space
LOCAL_TRAIN_SRC = ${TMPDIR}/${LANGSTR}/train/${DATASET}.src
LOCAL_TRAIN_TRG = ${TMPDIR}/${LANGSTR}/train/${DATASET}.trg

TUNE_SRC  = ${WORKDIR}/tune/${TUNESET}.src
TUNE_TRG  = ${WORKDIR}/tune/${TUNESET}.trg

DEV_SRC   = ${WORKDIR}/val/${DEVSET}.src
DEV_TRG   = ${WORKDIR}/val/${DEVSET}.trg

TEST_SRC  = ${WORKDIR}/test/${TESTSET}.src
TEST_TRG  = ${WORKDIR}/test/${TESTSET}.trg

## heldout data directory (keep one set per data set)
HELDOUT_DIR  = ${WORKDIR}/heldout



MODEL     = ${DATASET}${TRAINSIZE}.${PRE_SRC}-${PRE_TRG}.${lastword ${SRCLANGS}}${lastword ${TRGLANGS}}
MODELTYPE = transformer
NR        = 1

MODEL_BASENAME = ${MODEL}.${MODELTYPE}.model${NR}
MODEL_VALIDLOG = ${MODEL}.${MODELTYPE}.valid${NR}.log
MODEL_TRAINLOG = ${MODEL}.${MODELTYPE}.train${NR}.log
MODEL_FINAL    = ${WORKDIR}/${MODEL_BASENAME}.npz.best-perplexity.npz
MODEL_VOCAB    = ${WORKDIR}/${MODEL}.vocab.yml
MODEL_DECODER  = ${MODEL_FINAL}.decoder.yml




## parameters for running Marian NMT

MARIAN_GPUS           = 0
MARIAN_EXTRA          = 
MARIAN_VALID_FREQ     = 10000
MARIAN_SAVE_FREQ      = ${MARIAN_VALID_FREQ}
MARIAN_DISP_FREQ      = ${MARIAN_VALID_FREQ}
MARIAN_EARLY_STOPPING = 10

MARIAN_DECODER_GPU    = -b 12 -n1 -d ${MARIAN_GPUS} --mini-batch 8 --maxi-batch 32 --maxi-batch-sort src
MARIAN_DECODER_CPU    = -b 12 -n1 --cpu-threads ${HPC_CORES} --mini-batch 8 --maxi-batch 32 --maxi-batch-sort src
MARIAN_DECODER_FLAGS = ${MARIAN_DECODER_GPU}

ifeq (${GPU},p100)
  MARIAN_WORKSPACE = 13000
else ifeq (${GPU},v100)
  MARIAN_WORKSPACE = 30000
else
  MARIAN_WORKSPACE = 10000
endif


ifneq ("$(wildcard ${TRAIN_WEIGHTS})","")
	MARIAN_TRAIN_WEIGHTS = --data-weighting ${TRAIN_WEIGHTS}
endif


### training a model with Marian NMT
##
## NR allows to train several models for proper ensembling
## (with shared vocab)
##
## DANGER: if several models are started at the same time
## then there is some racing issue with creating the vocab!

ifdef NR
  SEED=${NR}${NR}${NR}${NR}
else
  SEED=1234
endif

